{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining the neural network class\n",
    "import numpy\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class neuralNetwork:\n",
    "    # function for initializing neural network\n",
    "    # The \"self\" is used to represent the instance of the class. \n",
    "    # With this keyword, you can access the attributes and methods of the class in python.\n",
    "    def __init__(self, inputnodes, hiddennodes, outputnodes, learningrate):\n",
    "        # set number of nodes in each nodes\n",
    "        self.inodes = inputnodes\n",
    "        self.hnodes = hiddennodes\n",
    "        self.onodes = outputnodes\n",
    "        self.lr = learningrate\n",
    "\n",
    "        # Linking the weight matrices, wih, who\n",
    "        # wih - weight linked with input and hidden layer\n",
    "        # who = wieight linked with output and hidden layer\n",
    "        # weights inside the arrays are w_i_j, where link is from node i to node j in the next layer\n",
    "        # w11 w21 like input to hidden 1, input 2 to hidden 1.\n",
    "        # w12 w22 etc visualized in paper. \n",
    "        self.wih = numpy.random.normal(0.0, pow(self.hnodes, -0.5),(self.hnodes, self.inodes))\n",
    "        self.who = numpy.random.normal(0.0, pow(self.onodes, -0.5),(self.onodes, self.hnodes))\n",
    "    \n",
    "        # updated code to initialize the weights\n",
    "        #self.wih = (numpy.random.normal(0.0, pow(self.hnodes, -0.5), (self.hnodes, self.inodes)))        \n",
    "        #self.who = (numpy.random.normal(0.0, pow(self.onodes, -0.5), (self.onodes, self.hnodes)))        \n",
    "\n",
    "        # Using the sigmoid function as the activation function\n",
    "        import scipy.special\n",
    "        self.activation_function = lambda x: scipy.special.expit(x)\n",
    "        # lambda is a special way of declaring functions which is anonymous(nameless) - \n",
    "        # it takes x as the input and returns the sigmoid function output using expit()\n",
    "\n",
    "        pass\n",
    "        #The pass statement is used as a placeholder for future code. \n",
    "        # When the pass statement is executed, nothing happens, \n",
    "        # but you avoid getting an error when empty code is not allowed. \n",
    "        # Empty code is not allowed in loops, function definitions, class definitions, or in if statements.\n",
    "        \n",
    "\n",
    "    # function for training the network\n",
    "    def train(self, input_list, target_list):\n",
    "        # feeding forward the signal from the input to the final layer\n",
    "        # convert the input and target list into a 2D matrix and transpose\n",
    "        inputs = numpy.array(input_list, ndmin=2).T\n",
    "\n",
    "        # target list contains the answer to the training data in a list for training the network\n",
    "        targets = numpy.array(target_list, ndmin=2).T\n",
    "\n",
    "        #calculate signals emerging in the hidden layer from the input\n",
    "        hidden_inputs = numpy.dot(self.wih, inputs)\n",
    "\n",
    "        #calculate signals generated by the hidden layer\n",
    "        hidden_outputs = self.activation_function(hidden_inputs)\n",
    "\n",
    "        #calculate signals generated into the final layer\n",
    "        final_inputs = numpy.dot(self.who, hidden_outputs)\n",
    "\n",
    "        #calculate signals emerging from the final layer (activation sheets by firing neurons)\n",
    "        final_outputs = self.activation_function(final_inputs)\n",
    "\n",
    "        #calculating the error between the target and actual for the neural network\n",
    "        output_errors = targets - final_outputs\n",
    "        \n",
    "        # updating the weights according to the error for optimization and weight tuning\n",
    "        # the output errors found above is the hidden layer errors.\n",
    "        # the output/hidden layer errors need to be split by weight\n",
    "        # then the split weight should be recombined with the hidden nodes to adjust/tune the network\n",
    "        hidden_errors = numpy.dot(self.who.T, output_errors)\n",
    "\n",
    "        # update the weights for the links between the hidden and output layer nodes\n",
    "        self.who += self.lr  * numpy.dot((output_errors * final_outputs * (1.0 - final_outputs)), numpy.transpose(hidden_outputs))\n",
    "    \n",
    "\n",
    "        # update the weights for the links between the input and the hidden  layer nodes\n",
    "        self.wih += self.lr * numpy.dot((hidden_errors * hidden_outputs * (1.0 - hidden_outputs)), numpy.transpose(inputs))\n",
    "\n",
    "    # function for implementing queries for the neural network\n",
    "    # takes input to a network and returns the output of the network\n",
    "    def query(self, input_list):\n",
    "        # following will use numpy to convert the inputs into 2D array and then transpose with .T\n",
    "        inputs = numpy.array(input_list, ndmin=2).T\n",
    "\n",
    "        # calculate the signals emerging in the hidden layer from the inputs\n",
    "        hidden_inputs = numpy.dot(self.wih, inputs)\n",
    "\n",
    "        # calculate signals emerging from the hidden layer neurons to the next layer\n",
    "        hidden_outputs = self.activation_function(hidden_inputs)\n",
    "\n",
    "        # calculate the signals into the final output layer - will act as inputs to the final layer\n",
    "        final_inputs = numpy.dot(self.who, hidden_outputs)\n",
    "\n",
    "        #calculate the signals emerging from the final output layer\n",
    "        final_outputs = self.activation_function(final_inputs)\n",
    "\n",
    "        return final_outputs\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# enter some small node and learning rate values for testing the network classes\n",
    "\n",
    "input_nodes = 784\n",
    "#input dataset was 28*28 pixels =784 nodes that make up the handwritten image dataset\n",
    "output_nodes = 10\n",
    "# by changing the output nodes the the number of predicted output class will also change.\n",
    "\n",
    "hidden_nodes = 100\n",
    "# hidden nodes selected less than input nodes because the eural network should select features which can be shorter than the input dataset\n",
    "# chossing smaller value helps the network to summarize the key features of the dataset\n",
    "# output nodes are 10\n",
    "# so selecting 100 seems enough for the network to express the features. \n",
    "# selecting smaller values may lead the network to not find enough explanatory features\n",
    "learning_rate = 0.3\n",
    "\n",
    "# creating instance of the neural network class\n",
    "n = neuralNetwork(input_nodes, hidden_nodes, output_nodes, learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#n.query([1.0, 0.5, -1.5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.15650766, 0.63711432, 0.1556899 ],\n",
       "       [0.45053378, 0.39166451, 0.56139994],\n",
       "       [0.91783136, 0.66608566, 0.77482734]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# use numpy to generate matrices of weight values.\n",
    "# at first assign very low weight values, for instance between 0 and 1. \n",
    "\n",
    "import numpy\n",
    "\n",
    "numpy.random.rand(3, 3) \n",
    "# 3*3 matirix with random numbers betwen 0 and 1 for weight values. \n",
    "# 3 nodes assigned previously so 3*3 matrix used.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# at first open the dataset from the local path in read only mode\n",
    "training_data_file = open(\"mnist_dataset/mnist_train.csv\", 'r')\n",
    "\n",
    "# use redlines() to read each record at a time as a list.append\n",
    "training_data_list = training_data_file.readlines()\n",
    "\n",
    "# close the dataset to avoid conflict elsewhere\n",
    "training_data_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "60000"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(training_data_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,38,43,105,255,253,253,253,253,253,174,6,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,43,139,224,226,252,253,252,252,252,252,252,252,158,14,0,0,0,0,0,0,0,0,0,0,0,0,0,0,178,252,252,252,252,253,252,252,252,252,252,252,252,59,0,0,0,0,0,0,0,0,0,0,0,0,0,0,109,252,252,230,132,133,132,132,189,252,252,252,252,59,0,0,0,0,0,0,0,0,0,0,0,0,0,0,4,29,29,24,0,0,0,0,14,226,252,252,172,7,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,85,243,252,252,144,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,88,189,252,252,252,14,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,91,212,247,252,252,252,204,9,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,32,125,193,193,193,253,252,252,252,238,102,28,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,45,222,252,252,252,252,253,252,252,252,177,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,45,223,253,253,253,253,255,253,253,253,253,74,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,31,123,52,44,44,44,44,143,252,252,74,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,15,252,252,74,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,86,252,252,74,0,0,0,0,0,0,0,0,0,0,0,0,0,0,5,75,9,0,0,0,0,0,0,98,242,252,252,74,0,0,0,0,0,0,0,0,0,0,0,0,0,61,183,252,29,0,0,0,0,18,92,239,252,252,243,65,0,0,0,0,0,0,0,0,0,0,0,0,0,208,252,252,147,134,134,134,134,203,253,252,252,188,83,0,0,0,0,0,0,0,0,0,0,0,0,0,0,208,252,252,252,252,252,252,252,252,253,230,153,8,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,49,157,252,252,252,252,252,217,207,146,45,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,7,103,235,252,172,103,24,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0\\n'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data_list[7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "import matplotlib.pyplot\n",
    "\n",
    "\n",
    "# Split the values of the first list based on commas\n",
    "#all_values = training_data_list[0].split(',')\n",
    "\n",
    "# remove the first label value and convert the remaining color pixel values into a numeric matrix\n",
    "#image_array = numpy.asfarray(all_values[1:]).reshape((28, 28))\n",
    "\n",
    "# now show only the image array\n",
    "#matplotlib.pyplot.imshow(image_array, cmap=\"Greys\", interpolation='None')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scale the input range from 0-255 to 0.01-1.00 using formula\n",
    "# re scaled and shaped the smaller mnist dataset\n",
    "\n",
    "#scaled_input = (numpy.asfarray(all_values[1:]) / 255 * 0.99) + 0.01\n",
    "#print(scaled_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the network fires neuron at the label which has the greates prediction accuracy. \n",
    "# for example if the output is 0, then the oth neuron will fire and remaining will stay silent. \n",
    "# there can also be uncertainities if 2 handwrien digits seem similar like 4 and 9\n",
    "\n",
    "# setting 10 output nodes for 10 digits\n",
    "#onodes = 10\n",
    "\n",
    "#setting all the nodes value to 0.01\n",
    "#targets = numpy.zeros(onodes) + 0.01\n",
    "\n",
    "# the first digit is zero, so setting the first digit to fire first neuron with highest value\n",
    "#targets[int(all_values[0])] = 0.99"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training the network with the whole dataset\n",
    "\n",
    "for record in training_data_list:\n",
    "    #split based on comma\n",
    "    all_values = record.split(',')\n",
    "\n",
    "    # scale and shift the inputs between 0.01 to 0.99\n",
    "    inputs = (numpy.asfarray(all_values[1:]) / 255.0 * 0.99) + 0.01\n",
    "\n",
    "    # create targets of output values (all neurons will fire 0.01 except the one hat is selected)\n",
    "    # converting all the output nodes to 0,01 \n",
    "    targets = numpy.zeros(output_nodes) + 0.01\n",
    "\n",
    "    # converting the ones with the higest similarity to 0.99\n",
    "    targets[int(all_values[0])] = 0.99\n",
    "\n",
    "    #training the network with inputs from the dataset and the target containing the out nodes\n",
    "    n.train(inputs, targets)\n",
    "    pass\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing phase for the neural network\n",
    "# first load the mnist test dataset\n",
    "test_data_file = open('mnist_dataset/mnist_test.csv', 'r')\n",
    "test_data_list = test_data_file.readlines()\n",
    "test_data_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get the first data file\n",
    "#all_values = test_data_list[0].split(',')\n",
    "#print(all_values[0])\n",
    "# the oth index of the train dataset contains 7\n",
    "len(test_data_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import numpy\n",
    "#import matplotlib.pyplot\n",
    "#image_array = numpy.asfarray(all_values[1:]).reshape((28, 28))\n",
    "#matplotlib.pyplot.imshow(image_array, cmap = \"Greys\")\n",
    "\n",
    "# visualizing the oth index handwritten image from the"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# n.query((numpy.asfarray(all_values[1:]) / 255.0 * 0.99) + 0.01)\n",
    "\n",
    "# testing if the neuron fires greatest value for the 7th output node to check the network capability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing the neural network with the whole dataset\n",
    "#plt.axis([0, 10, 0, 10])\n",
    "# first creating a list of scorecard to check its scores later on\n",
    "scorecard = []\n",
    "\n",
    "# going through the enire test dataset from the opened mnist test small above\n",
    "for record in test_data_list:\n",
    "    # spliting depending on commas\n",
    "    all_values = record.split(',')\n",
    "\n",
    "    # correct answer is the first value which lies in the first column of the dataset\n",
    "    correct_label = int(all_values[0])\n",
    "    #print(correct_label, \"correct label\")\n",
    "\n",
    "    # scaling the other image pixel inputs between 0.01 to 0.99\n",
    "    inputs = (numpy.asfarray(all_values[1:]) / 255.0 * 0.99) + 0.01\n",
    "\n",
    "    # query the network\n",
    "    # it will assign and distribute the weights and prepare the activation map from the network\n",
    "    outputs = n.query(inputs)\n",
    "\n",
    "    # the index of the highest value corresponds to the label\n",
    "    # the network will show the max value on the neuron that has the highest match with the existing data point\n",
    "    # numpy argmax() finds the largest value in an array\n",
    "    label = numpy.argmax(outputs)\n",
    "    # it will also print the output provided by the network\n",
    "    # print(label, \"network's answer\")\n",
    "    # append correct or incorrect to list\n",
    "    # the correct labels are available in the correct_label column\n",
    "    # the network predicted labels are in the label column\n",
    "    # the following condition will match the labels to determine if the network predicted the correct label\n",
    "    if (label == correct_label):\n",
    "        # add 1 if the network gives correct answer\n",
    "        scorecard.append(1)\n",
    "    else:\n",
    "        # add 0 if incorrect answer from the network\n",
    "        scorecard.append(0)\n",
    "        # from this a list of values will be obtained that will show the number of correct and incorrect predictions made by our network\n",
    "        # it can be used to determine the network accuracy\n",
    "        pass\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(scorecard)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Network performance = 94.83 %\n"
     ]
    }
   ],
   "source": [
    "# Lets now print the performance accuracy percentage\n",
    "scorecard_array = numpy.asarray(scorecard)\n",
    "# converted the scorecard to array to individually count the number of elements in the array and operate on each elements\n",
    "print(\"Network performance =\", (scorecard_array.sum() / scorecard_array.size) * 100, \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For learning rate= 0.01\n",
      "Network performance = 91.97 %\n",
      "For learning rate= 0.02\n",
      "Network performance = 93.38 %\n",
      "For learning rate= 0.03\n",
      "Network performance = 93.77 %\n",
      "For learning rate= 0.04\n",
      "Network performance = 94.42 %\n",
      "For learning rate= 0.05\n",
      "Network performance = 94.75 %\n",
      "For learning rate= 0.06\n",
      "Network performance = 94.67 %\n",
      "For learning rate= 0.11\n",
      "Network performance = 95.41 %\n",
      "For learning rate= 0.21\n",
      "Network performance = 94.83 %\n",
      "For learning rate= 0.31\n",
      "Network performance = 93.95 %\n",
      "For learning rate= 0.41\n",
      "Network performance = 92.23 %\n",
      "For learning rate= 0.51\n",
      "Network performance = 91.38 %\n",
      "For learning rate= 0.61\n",
      "Network performance = 90.14 %\n",
      "For learning rate= 0.71\n",
      "Network performance = 85.39999999999999 %\n",
      "For learning rate= 0.81\n",
      "Network performance = 85.0 %\n",
      "For learning rate= 0.91\n",
      "Network performance = 84.85000000000001 %\n",
      "For learning rate= 0.99\n",
      "Network performance = 84.53 %\n"
     ]
    }
   ],
   "source": [
    "for lr in [0.01, 0.02, 0.03, 0.04, 0.05, 0.06, 0.11, 0.21, 0.31, 0.41, 0.51, 0.61, 0.71, 0.81, 0.91, 0.99]:\n",
    "    n_test_lr = neuralNetwork(input_nodes, hidden_nodes, output_nodes, lr)\n",
    "    print(\"For learning rate=\", lr)\n",
    "    for record in training_data_list:\n",
    "        all_values = record.split(',')\n",
    "\n",
    "        inputs = (numpy.asfarray(all_values[1:]) / 255.0 * 0.99) + 0.01\n",
    "\n",
    "        targets = numpy.zeros(output_nodes) + 0.01\n",
    "\n",
    "        targets[int(all_values[0])] = 0.99\n",
    "\n",
    "        n_test_lr.train(inputs, targets)\n",
    "        pass\n",
    "\n",
    "    scorecard = []\n",
    "\n",
    "    for record in test_data_list:\n",
    "\n",
    "        all_values = record.split(',')\n",
    "\n",
    "        correct_label = int(all_values[0])\n",
    "   \n",
    "        inputs = (numpy.asfarray(all_values[1:]) / 255.0 * 0.99) + 0.01\n",
    "\n",
    "        outputs = n_test_lr.query(inputs)\n",
    "\n",
    "        label = numpy.argmax(outputs)\n",
    "\n",
    "        if (label == correct_label):\n",
    "            scorecard.append(1)\n",
    "        else:\n",
    "            scorecard.append(0)\n",
    "\n",
    "        scorecard_array = numpy.asarray(scorecard)\n",
    "# converted the scorecard to array to individually count the number of elements in the array and operate on each elements\n",
    "    print(\"Network performance =\", (scorecard_array.sum() / scorecard_array.size) * 100, \"%\")\n",
    "    pass\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.2 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "723d4b7bc280cd31fdada53ad6420192b9a3a8d60631096143cc718cb9440dc1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
