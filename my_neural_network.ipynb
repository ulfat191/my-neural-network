{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining the neural network class\n",
    "import numpy\n",
    "\n",
    "class neuralNetwork:\n",
    "    # function for initializing neural network\n",
    "    # The \"self\" is used to represent the instance of the class. \n",
    "    # With this keyword, you can access the attributes and methods of the class in python.\n",
    "    def __init__(self, inputnodes, hiddennodes, outputnodes, learningrate):\n",
    "        # set number of nodes in each nodes\n",
    "        self.inodes = inputnodes\n",
    "        self.hnodes = hiddennodes\n",
    "        self.onodes = outputnodes\n",
    "        self.lr = learningrate\n",
    "\n",
    "        # Linking the weight matrices, wih, who\n",
    "        # wih - weight linked with input and hidden layer\n",
    "        # who = wieight linked with output and hidden layer\n",
    "        # weights inside the arrays are w_i_j, where link is from node i to node j in the next layer\n",
    "        # w11 w21 like input to hidden 1, input 2 to hidden 1.\n",
    "        # w12 w22 etc visualized in paper. \n",
    "        self.wih = numpy.random.normal(0.0, pow(self.hnodes, -0.5),(self.hnodes, self.inodes))\n",
    "        self.who = numpy.random.normal(0.0, pow(self.onodes, -0.5),(self.onodes, self.hnodes))\n",
    "    \n",
    "        # updated code to initialize the weights\n",
    "        #self.wih = (numpy.random.normal(0.0, pow(self.hnodes, -0.5), (self.hnodes, self.inodes)))        \n",
    "        #self.who = (numpy.random.normal(0.0, pow(self.onodes, -0.5), (self.onodes, self.hnodes)))        \n",
    "\n",
    "        # Using the sigmoid function as the activation function\n",
    "        import scipy.special\n",
    "        self.activation_function = lambda x: scipy.special.expit(x)\n",
    "        # lambda is a special way of declaring functions which is anonymous(nameless) - \n",
    "        # it takes x as the input and returns the sigmoid function output using expit()\n",
    "\n",
    "        pass\n",
    "        #The pass statement is used as a placeholder for future code. \n",
    "        # When the pass statement is executed, nothing happens, \n",
    "        # but you avoid getting an error when empty code is not allowed. \n",
    "        # Empty code is not allowed in loops, function definitions, class definitions, or in if statements.\n",
    "        \n",
    "\n",
    "    # function for training the network\n",
    "    def train(self, input_list, target_list):\n",
    "        # feeding forward the signal from the input to the final layer\n",
    "        # convert the input and target list into a 2D matrix and transpose\n",
    "        inputs = numpy.array(input_list, ndmin=2).T\n",
    "\n",
    "        # target list contains the training data in a list for training the network\n",
    "        targets = numpy.array(target_list, ndmin=2).T\n",
    "\n",
    "        #calculate signals emerging in the hidden layer from the input\n",
    "        hidden_inputs = numpy.dot(self.wih, inputs)\n",
    "\n",
    "        #calculate signals generated by the hidden layer\n",
    "        hidden_outputs = self.activation_function(hidden_inputs)\n",
    "\n",
    "        #calculate signals generated into the final layer\n",
    "        final_inputs = numpy.dot(self.who, hidden_outputs)\n",
    "\n",
    "        #calculate signals emerging from the final layer (activation sheets by firing neurons)\n",
    "        final_outputs = self.activation_function(final_inputs)\n",
    "\n",
    "        #calculating the error between the target and actual for the neural network\n",
    "        output_errors = targets - final_outputs\n",
    "        \n",
    "        # updating the weights according to the error for optimization and weight tuning\n",
    "        # the output errors found above is the hidden layer errors.\n",
    "        # the output/hidden layer errors need to be split by weight\n",
    "        # then the split weight should be recombined with the hidden nodes to adjust/tune the network\n",
    "        hidden_errors = numpy.dot(self.who.T, output_errors)\n",
    "\n",
    "        # update the weights for the links between the hidden and output layer nodes\n",
    "        self.who += self.lr  * numpy.dot((output_errors * final_outputs * (1.0 - final_outputs)), numpy.transpose(hidden_outputs))\n",
    "    \n",
    "\n",
    "        # update the weights for the links between the input and the hidden  layer nodes\n",
    "        self.wih += self.lr * numpy.dot((hidden_errors * hidden_outputs * (1.0 - hidden_outputs)), numpy.transpose(inputs))\n",
    "\n",
    "    # function for implementing queries for the neural network\n",
    "    # takes input to a network and returns the output of the network\n",
    "    def query(self, input_list):\n",
    "        # following will use numpy to convert the inputs into 2D array and then transpose with .T\n",
    "        inputs = numpy.array(input_list, ndmin=2).T\n",
    "\n",
    "        # calculate the signals emerging in the hidden layer from the inputs\n",
    "        hidden_inputs = numpy.dot(self.wih, inputs)\n",
    "\n",
    "        # calculate signals emerging from the hidden layer neurons to the next layer\n",
    "        hidden_outputs = self.activation_function(hidden_inputs)\n",
    "\n",
    "        # calculate the signals into the final output layer - will act as inputs to the final layer\n",
    "        final_inputs = numpy.dot(self.who, hidden_outputs)\n",
    "\n",
    "        #calculate the signals emerging from the final output layer\n",
    "        final_outputs = self.activation_function(final_inputs)\n",
    "\n",
    "        return final_outputs\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# enter some small node and learning rate values for testing the network classes\n",
    "\n",
    "input_nodes = 784\n",
    "output_nodes = 10\n",
    "# by changing the output nodes the the number of predicted output class will also change.\n",
    "\n",
    "hidden_nodes = 100\n",
    "learning_rate = 0.3\n",
    "\n",
    "# creating instance of the neural network class\n",
    "n = neuralNetwork(input_nodes, hidden_nodes, output_nodes, learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "#n.query([1.0, 0.5, -1.5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.57938583, 0.22931399, 0.85007804],\n",
       "       [0.29701272, 0.41437331, 0.98384946],\n",
       "       [0.86824328, 0.5180835 , 0.59385753]])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# use numpy to generate matrices of weight values.\n",
    "# at first assign very low weight values, for instance between 0 and 1. \n",
    "\n",
    "import numpy\n",
    "\n",
    "numpy.random.rand(3, 3) \n",
    "# 3*3 matirix with random numbers betwen 0 and 1 for weight values. \n",
    "# 3 nodes assigned previously so 3*3 matrix used.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# at first open the dataset from the local path in read only mode\n",
    "training_data_file = open(\"mnist_dataset/mnist_train_100.csv\", 'r')\n",
    "\n",
    "# use redlines() to read each record at a time as a list.append\n",
    "training_data_list = training_data_file.readlines()\n",
    "\n",
    "# close the dataset to avoid conflict elsewhere\n",
    "training_data_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(training_data_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,38,43,105,255,253,253,253,253,253,174,6,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,43,139,224,226,252,253,252,252,252,252,252,252,158,14,0,0,0,0,0,0,0,0,0,0,0,0,0,0,178,252,252,252,252,253,252,252,252,252,252,252,252,59,0,0,0,0,0,0,0,0,0,0,0,0,0,0,109,252,252,230,132,133,132,132,189,252,252,252,252,59,0,0,0,0,0,0,0,0,0,0,0,0,0,0,4,29,29,24,0,0,0,0,14,226,252,252,172,7,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,85,243,252,252,144,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,88,189,252,252,252,14,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,91,212,247,252,252,252,204,9,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,32,125,193,193,193,253,252,252,252,238,102,28,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,45,222,252,252,252,252,253,252,252,252,177,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,45,223,253,253,253,253,255,253,253,253,253,74,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,31,123,52,44,44,44,44,143,252,252,74,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,15,252,252,74,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,86,252,252,74,0,0,0,0,0,0,0,0,0,0,0,0,0,0,5,75,9,0,0,0,0,0,0,98,242,252,252,74,0,0,0,0,0,0,0,0,0,0,0,0,0,61,183,252,29,0,0,0,0,18,92,239,252,252,243,65,0,0,0,0,0,0,0,0,0,0,0,0,0,208,252,252,147,134,134,134,134,203,253,252,252,188,83,0,0,0,0,0,0,0,0,0,0,0,0,0,0,208,252,252,252,252,252,252,252,252,253,230,153,8,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,49,157,252,252,252,252,252,217,207,146,45,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,7,103,235,252,172,103,24,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0\\n'"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data_list[7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "import matplotlib.pyplot\n",
    "\n",
    "\n",
    "# Split the values of the first list based on commas\n",
    "#all_values = training_data_list[0].split(',')\n",
    "\n",
    "# remove the first label value and convert the remaining color pixel values into a numeric matrix\n",
    "#image_array = numpy.asfarray(all_values[1:]).reshape((28, 28))\n",
    "\n",
    "# now show only the image array\n",
    "#matplotlib.pyplot.imshow(image_array, cmap=\"Greys\", interpolation='None')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scale the input range from 0-255 to 0.01-1.00 using formula\n",
    "# re scaled and shaped the smaller mnist dataset\n",
    "\n",
    "#scaled_input = (numpy.asfarray(all_values[1:]) / 255 * 0.99) + 0.01\n",
    "#print(scaled_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the network fires neuron at the label which has the greates prediction accuracy. \n",
    "# for example if the output is 0, then the oth neuron will fire and remaining will stay silent. \n",
    "# there can also be uncertainities if 2 handwrien digits seem similar like 4 and 9\n",
    "\n",
    "# setting 10 output nodes for 10 digits\n",
    "#onodes = 10\n",
    "\n",
    "#setting all the nodes value to 0.01\n",
    "#targets = numpy.zeros(onodes) + 0.01\n",
    "\n",
    "# the first digit is zero, so setting the first digit to fire first neuron with highest value\n",
    "#targets[int(all_values[0])] = 0.99"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training the network with the whole dataset\n",
    "\n",
    "for record in training_data_list:\n",
    "    #split based on comma\n",
    "    all_values = record.split(',')\n",
    "\n",
    "    # scale and shift the inputs\n",
    "    inputs = (numpy.asfarray(all_values[1:]) / 255.0 * 0.99) + 0.01\n",
    "\n",
    "    # create targets of output values (all neurons will fire 0.01 except the one hat is selected)\n",
    "    targets = numpy.zeros(output_nodes) + 0.01\n",
    "\n",
    "    targets[int(all_values[0])] = 0.99\n",
    "    n.train(inputs, targets)\n",
    "    pass\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.2 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "723d4b7bc280cd31fdada53ad6420192b9a3a8d60631096143cc718cb9440dc1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
